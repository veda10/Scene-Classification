{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "q2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "rquheDGHE62b",
        "colab_type": "code",
        "outputId": "7457716c-3601-42f2-f81c-4d0a0e756a7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# mounting with dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "w8MW8GYrFAej",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# importing libraries\n",
        "from __future__ import print_function\n",
        "from __future__ import division\n",
        "\n",
        "import os,cv2,scipy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import copy\n",
        "import torchvision.transforms.functional as TF\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import io as sio\n",
        "from scipy import ndimage, misc\n",
        "import csv\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nlAFAGgMJkW9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data_dir = \"/content/drive/My Drive/HW3_data\"\n",
        "\n",
        "#setting alexnet model \n",
        "model_name = \"alexnet\"\n",
        "\n",
        "# Number of classes in the dataset\n",
        "num_classes = 8\n",
        "\n",
        "# Batch size for training\n",
        "batch_size = 8\n",
        "\n",
        "# Number of training epochs\n",
        "num_epochs = 15\n",
        "\n",
        "# the last layer parameters are updated, the others remain fixed.\n",
        "extract_features = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Xp4mCMAEZ9ya",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Reading Images\n",
        "def readImages(imagesPath, imagesCount):\n",
        "  images = []\n",
        "  \n",
        "  #Looping through all the images \n",
        "  for i in range(imagesCount):\n",
        "    #Reading image paths\n",
        "    path = imagesPath + '/' + str(i+1) + '.jpg'\n",
        "    \n",
        "    #reading the image in the dataset from path\n",
        "    img = scipy.misc.imread(path)\n",
        "    \n",
        "    #appending all the images in the images array\n",
        "    images.append(img)\n",
        "    \n",
        "  #converting it to a numpy array\n",
        "  images = np.array(images)\n",
        "  \n",
        "  #Image Dimensions\n",
        "  print(images.shape)\n",
        "  return images"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ml56mttlaIae",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# storing all the labels in labels variable\n",
        "def readLabels(labelsPath):\n",
        "  \n",
        "  #opening the csv file\n",
        "\twith open(labelsPath,'rU') as csvfile:\n",
        "    #reading the csv file and converting to list\n",
        "\t\tcsvfile = csv.reader(csvfile, delimiter=',')\n",
        "\t\tcsvdata = list(csvfile)\n",
        "    \n",
        "    #saving the labels in labels variable in integer format\n",
        "\t\tlabels = map(int, csvdata[0])\n",
        "    \n",
        "    #returning the list\n",
        "\t\treturn list(labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wFlYPG5kaTVy",
        "colab_type": "code",
        "outputId": "f345c183-ace6-4e5e-83b8-c31eacf5ea1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "cell_type": "code",
      "source": [
        "# train image path\n",
        "training_images_path = '/content/drive/My Drive/HW3_data/train'\n",
        "\n",
        "# reading images in train dataset\n",
        "train_images = readImages(training_images_path, 1888)\n",
        "\n",
        "# dimensions of train_images variable\n",
        "train_images.shape\n",
        "\n",
        "# path of train data labels\n",
        "training_labels_path = '/content/drive/My Drive/HW3_data/train_labels.csv'\n",
        "\n",
        "# saving the labels in train_labels\n",
        "train_labels = readLabels(training_labels_path)\n",
        "\n",
        "\n",
        "\n",
        "# test images path in the dataset\n",
        "testing_images_path = '/content/drive/My Drive/HW3_data/test'\n",
        "\n",
        "# reading images in test dataset\n",
        "test_images = readImages(testing_images_path, 800)\n",
        "\n",
        "# dimensions of test_images variable\n",
        "test_images.shape\n",
        "\n",
        "# path of test data labels\n",
        "testing_labels_path = '/content/drive/My Drive/HW3_data/test_labels.csv'\n",
        "\n",
        "# saving the labels in test_labels\n",
        "test_labels = readLabels(testing_labels_path)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: DeprecationWarning: `imread` is deprecated!\n",
            "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``imageio.imread`` instead.\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(1888, 256, 256, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: DeprecationWarning: 'U' mode is deprecated\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(800, 256, 256, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mjZUWUrCKWN_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# we want all of the other parameters to not require gradient\n",
        "# therefore setting the parameter false\n",
        "def set_parameter_requires_grad(model, feature_extracting):\n",
        "\n",
        "    #feature_extracting is true\n",
        "    if feature_extracting:\n",
        "        for param in model.parameters():\n",
        "            #By default, when we load a pretrained model all of the parameters have gradients. That is we have param.requires_grad=True\n",
        "            param.requires_grad = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pKIH1iaXFQ5I",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# pretraining the alexnet model\n",
        "alexnet = models.alexnet(pretrained=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tyYL5ghCFU8T",
        "colab_type": "code",
        "outputId": "d9353d72-4e9e-4949-ce80-59333449f181",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 492
        }
      },
      "cell_type": "code",
      "source": [
        "# Alexnet Achitecture\n",
        "alexnet.eval()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AlexNet(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
              "    (1): ReLU(inplace)\n",
              "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "    (4): ReLU(inplace)\n",
              "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (7): ReLU(inplace)\n",
              "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (9): ReLU(inplace)\n",
              "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU(inplace)\n",
              "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
              "  (classifier): Sequential(\n",
              "    (0): Dropout(p=0.5)\n",
              "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
              "    (2): ReLU(inplace)\n",
              "    (3): Dropout(p=0.5)\n",
              "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    (5): ReLU(inplace)\n",
              "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "metadata": {
        "id": "JmFtVP2UFVBz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# initializing the Alexnet model\n",
        "def initialize_model(model_name, num_classes, extract_features, use_pretrained=True):\n",
        "    input_size = 0\n",
        "    model_features = None\n",
        "    \n",
        "    if model_name == \"alexnet\":\n",
        "        #alexnet is trained with imagenet dataset. \n",
        "        model_features = models.alexnet(pretrained=use_pretrained)\n",
        "        \n",
        "        #removing gradients so that we can use for feature extraction\n",
        "        set_parameter_requires_grad(model_features, extract_features)\n",
        "        num_ftrs = model_features.classifier[6].in_features\n",
        "  \n",
        "        #we print the model architecture in such a way that we see the model output comes from the 6th layer of the classifier\n",
        "        # output layer has 8 layers because we have 8 classes\n",
        "        model_features.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
        "        input_size = 224\n",
        "\n",
        "\n",
        "    return model_features, input_size\n",
        "\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QP33rGDu8SXs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# saving training labels data \n",
        "train_labels = np.array(pd.read_csv('/content/drive/My Drive/HW3_data/train_labels.csv',header =None))[0]\n",
        "# saving testing labels data\n",
        "test_labels = np.array(pd.read_csv('/content/drive/My Drive/HW3_data/test_labels.csv',header =None))[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-kvz5g4XTbDj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Face Landmarks Dataset\n",
        "class FaceLandmarksDataset(Dataset):\n",
        "\n",
        "    def __init__(self,root_dir,total_count, csv_file, transform=None):\n",
        "              #csv_file (string): Path to the csv file with annotations.\n",
        "              #root_dir (string): Directory with all the images.\n",
        "              #transform (callable, optional): Optional transform to be applied on a sample.\n",
        "        \n",
        "        self.transform = transform\n",
        "        self.csv_file = np.array(pd.read_csv(csv_file,header=None))[0]\n",
        "        self.root_dir = root_dir\n",
        "        self.total_count = total_count\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.total_count\n",
        "\n",
        "    def __getitem__(self, image_no):\n",
        "        #joining the path\n",
        "        img_name = os.path.join(self.root_dir,\n",
        "                                str(image_no+1)+\".jpg\")\n",
        "        image = Image.open(img_name)\n",
        "        \n",
        "        #creating dictionary with image's corresponding label\n",
        "        image_label_dict = {\n",
        "            'image': image,\n",
        "            'label': self.csv_file[image_no]-1\n",
        "           }\n",
        "\n",
        "        if self.transform:\n",
        "            image_label_dict['image'] = self.transform(image_label_dict['image'])\n",
        "\n",
        "        return image_label_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d-KdkZBEUBcD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Load Data\n",
        "# Data augmentation and normalization for training\n",
        "trainDataset = FaceLandmarksDataset('/content/drive/My Drive/HW3_data/train',1888,'/content/drive/My Drive/HW3_data/train_labels.csv',transforms.Compose([\n",
        "\n",
        "        #resizing\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "\n",
        "        #The images have to be loaded in to a range of [0, 1] and then normalized\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]))\n",
        "\n",
        "# Just normalization for validation\n",
        "testDataset = FaceLandmarksDataset('/content/drive/My Drive/HW3_data/test',800,'/content/drive/My Drive/HW3_data/test_labels.csv',transforms.Compose([\n",
        "        \n",
        "        #resizing\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        \n",
        "        #The images have to be loaded in to a range of [0, 1] and then normalized\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q6JbuZhCFVF6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Initialize the model for this run\n",
        "model_features, input_size = initialize_model(\"alexnet\", 8, extract_features, use_pretrained=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bR2cb0R_Lc9T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Initializing Datasets and Dataloaders as inputs for model\n",
        "# Create training and validation dataloaders\n",
        "trainDataloader = torch.utils.data.DataLoader(trainDataset,batch_size = 8, shuffle = True, num_workers=4)\n",
        "testDataloader = torch.utils.data.DataLoader(testDataset,batch_size = 8, shuffle = True, num_workers=4)\n",
        "Dataloaders_dict = {trainDataloader,testDataloader}\n",
        "\n",
        "# Detect if we have a GPU available\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mI4GhKz9O93b",
        "colab_type": "code",
        "outputId": "db2ae0dd-2f9a-43d7-a3d0-0160ba272139",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "cell_type": "code",
      "source": [
        "# Send the model to GPU\n",
        "model_features = model_features.to(device)\n",
        "\n",
        "# Gather the parameters to be optimized/updated in this run. If we are\n",
        "#  finetuning we will be updating all parameters. However, if we are\n",
        "#  doing feature extract method, we will only update the parameters\n",
        "#  that we have just initialized, i.e. the parameters with requires_grad\n",
        "#  is True.\n",
        "\n",
        "#Update intialized parameters\n",
        "params_to_update = model_features.parameters()\n",
        "print(\"Params to learn:\")\n",
        "\n",
        "if extract_features:\n",
        "    #parameters to be updated in this run \n",
        "    params_to_update = []\n",
        "    \n",
        "    #iterating through all the parameters\n",
        "    for name,param in model_features.named_parameters():\n",
        "        #check if gradient is true, if true then update and set to false\n",
        "        if param.requires_grad == True:\n",
        "            #saving all the parameters to be updated to params_to_update\n",
        "            params_to_update.append(param)\n",
        "            print(\"\\t\",name)\n",
        "else:\n",
        "    for name,param in model_features.named_parameters():\n",
        "        if param.requires_grad == True:\n",
        "            print(\"\\t\",name)\n",
        "\n",
        "# All parameters are being optimized\n",
        "optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Params to learn:\n",
            "\t classifier.6.weight\n",
            "\t classifier.6.bias\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dIsa0YHRV1ro",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Selecting Cross Entropy as the loss function\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wAGSFgIrLwCd",
        "colab_type": "code",
        "outputId": "50fcb13c-8cbc-4425-bafa-afa9af5f45dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6106
        }
      },
      "cell_type": "code",
      "source": [
        "# loop over the dataset multiple times\n",
        "for epoch in range(num_epochs):  \n",
        "# initializing with 0\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainDataloader, 0):\n",
        "        #get the inputs\n",
        "        inputs, labels = data['image'],data['label']\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        #zero the parameter gradients\n",
        "        optimizer_ft.zero_grad()\n",
        "        outputs = model_features(inputs)\n",
        "        \n",
        "        #calculating loss\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer_ft.step()\n",
        "\n",
        "        #print statistics\n",
        "        running_loss += loss.item()\n",
        "        \n",
        "        #print every 20 mini-batches\n",
        "        if i % 20 == 19:    \n",
        "            print('[%d, %5d] loss: %.3f' %(epoch + 1, i + 1, running_loss / 20))\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,    10] loss: 1.631\n",
            "[1,    20] loss: 0.765\n",
            "[1,    30] loss: 0.513\n",
            "[1,    40] loss: 0.473\n",
            "[1,    50] loss: 0.713\n",
            "[1,    60] loss: 0.413\n",
            "[1,    70] loss: 0.584\n",
            "[1,    80] loss: 0.351\n",
            "[1,    90] loss: 0.543\n",
            "[1,   100] loss: 0.420\n",
            "[1,   110] loss: 0.509\n",
            "[1,   120] loss: 0.432\n",
            "[1,   130] loss: 0.356\n",
            "[1,   140] loss: 0.416\n",
            "[1,   150] loss: 0.414\n",
            "[1,   160] loss: 0.602\n",
            "[1,   170] loss: 0.337\n",
            "[1,   180] loss: 0.473\n",
            "[1,   190] loss: 0.555\n",
            "[1,   200] loss: 0.599\n",
            "[1,   210] loss: 0.684\n",
            "[1,   220] loss: 0.429\n",
            "[1,   230] loss: 0.489\n",
            "[2,    10] loss: 0.470\n",
            "[2,    20] loss: 0.434\n",
            "[2,    30] loss: 0.316\n",
            "[2,    40] loss: 0.702\n",
            "[2,    50] loss: 0.219\n",
            "[2,    60] loss: 0.266\n",
            "[2,    70] loss: 0.372\n",
            "[2,    80] loss: 0.678\n",
            "[2,    90] loss: 0.322\n",
            "[2,   100] loss: 0.257\n",
            "[2,   110] loss: 0.408\n",
            "[2,   120] loss: 0.373\n",
            "[2,   130] loss: 0.628\n",
            "[2,   140] loss: 0.294\n",
            "[2,   150] loss: 0.337\n",
            "[2,   160] loss: 0.330\n",
            "[2,   170] loss: 0.517\n",
            "[2,   180] loss: 0.432\n",
            "[2,   190] loss: 0.212\n",
            "[2,   200] loss: 0.577\n",
            "[2,   210] loss: 0.310\n",
            "[2,   220] loss: 0.337\n",
            "[2,   230] loss: 0.335\n",
            "[3,    10] loss: 0.222\n",
            "[3,    20] loss: 0.285\n",
            "[3,    30] loss: 0.433\n",
            "[3,    40] loss: 0.562\n",
            "[3,    50] loss: 0.645\n",
            "[3,    60] loss: 0.413\n",
            "[3,    70] loss: 0.539\n",
            "[3,    80] loss: 0.476\n",
            "[3,    90] loss: 0.218\n",
            "[3,   100] loss: 0.426\n",
            "[3,   110] loss: 0.462\n",
            "[3,   120] loss: 0.475\n",
            "[3,   130] loss: 0.347\n",
            "[3,   140] loss: 0.287\n",
            "[3,   150] loss: 0.233\n",
            "[3,   160] loss: 0.417\n",
            "[3,   170] loss: 0.306\n",
            "[3,   180] loss: 0.322\n",
            "[3,   190] loss: 0.323\n",
            "[3,   200] loss: 0.229\n",
            "[3,   210] loss: 0.539\n",
            "[3,   220] loss: 0.316\n",
            "[3,   230] loss: 0.554\n",
            "[4,    10] loss: 0.386\n",
            "[4,    20] loss: 0.342\n",
            "[4,    30] loss: 0.317\n",
            "[4,    40] loss: 0.518\n",
            "[4,    50] loss: 0.440\n",
            "[4,    60] loss: 0.282\n",
            "[4,    70] loss: 0.469\n",
            "[4,    80] loss: 0.627\n",
            "[4,    90] loss: 0.269\n",
            "[4,   100] loss: 0.261\n",
            "[4,   110] loss: 0.289\n",
            "[4,   120] loss: 0.464\n",
            "[4,   130] loss: 0.208\n",
            "[4,   140] loss: 0.389\n",
            "[4,   150] loss: 0.366\n",
            "[4,   160] loss: 0.373\n",
            "[4,   170] loss: 0.332\n",
            "[4,   180] loss: 0.273\n",
            "[4,   190] loss: 0.410\n",
            "[4,   200] loss: 0.318\n",
            "[4,   210] loss: 0.283\n",
            "[4,   220] loss: 0.445\n",
            "[4,   230] loss: 0.477\n",
            "[5,    10] loss: 0.305\n",
            "[5,    20] loss: 0.259\n",
            "[5,    30] loss: 0.623\n",
            "[5,    40] loss: 0.304\n",
            "[5,    50] loss: 0.362\n",
            "[5,    60] loss: 0.220\n",
            "[5,    70] loss: 0.213\n",
            "[5,    80] loss: 0.461\n",
            "[5,    90] loss: 0.125\n",
            "[5,   100] loss: 0.148\n",
            "[5,   110] loss: 0.422\n",
            "[5,   120] loss: 0.371\n",
            "[5,   130] loss: 0.378\n",
            "[5,   140] loss: 0.535\n",
            "[5,   150] loss: 0.324\n",
            "[5,   160] loss: 0.328\n",
            "[5,   170] loss: 0.455\n",
            "[5,   180] loss: 0.571\n",
            "[5,   190] loss: 0.408\n",
            "[5,   200] loss: 0.327\n",
            "[5,   210] loss: 0.523\n",
            "[5,   220] loss: 0.313\n",
            "[5,   230] loss: 0.373\n",
            "[6,    10] loss: 0.280\n",
            "[6,    20] loss: 0.394\n",
            "[6,    30] loss: 0.379\n",
            "[6,    40] loss: 0.410\n",
            "[6,    50] loss: 0.239\n",
            "[6,    60] loss: 0.334\n",
            "[6,    70] loss: 0.208\n",
            "[6,    80] loss: 0.330\n",
            "[6,    90] loss: 0.413\n",
            "[6,   100] loss: 0.448\n",
            "[6,   110] loss: 0.191\n",
            "[6,   120] loss: 0.370\n",
            "[6,   130] loss: 0.316\n",
            "[6,   140] loss: 0.150\n",
            "[6,   150] loss: 0.362\n",
            "[6,   160] loss: 0.486\n",
            "[6,   170] loss: 0.367\n",
            "[6,   180] loss: 0.346\n",
            "[6,   190] loss: 0.447\n",
            "[6,   200] loss: 0.201\n",
            "[6,   210] loss: 0.358\n",
            "[6,   220] loss: 0.262\n",
            "[6,   230] loss: 0.391\n",
            "[7,    10] loss: 0.478\n",
            "[7,    20] loss: 0.338\n",
            "[7,    30] loss: 0.381\n",
            "[7,    40] loss: 0.388\n",
            "[7,    50] loss: 0.270\n",
            "[7,    60] loss: 0.427\n",
            "[7,    70] loss: 0.127\n",
            "[7,    80] loss: 0.177\n",
            "[7,    90] loss: 0.547\n",
            "[7,   100] loss: 0.276\n",
            "[7,   110] loss: 0.343\n",
            "[7,   120] loss: 0.121\n",
            "[7,   130] loss: 0.355\n",
            "[7,   140] loss: 0.423\n",
            "[7,   150] loss: 0.356\n",
            "[7,   160] loss: 0.242\n",
            "[7,   170] loss: 0.358\n",
            "[7,   180] loss: 0.340\n",
            "[7,   190] loss: 0.386\n",
            "[7,   200] loss: 0.174\n",
            "[7,   210] loss: 0.257\n",
            "[7,   220] loss: 0.531\n",
            "[7,   230] loss: 0.410\n",
            "[8,    10] loss: 0.312\n",
            "[8,    20] loss: 0.216\n",
            "[8,    30] loss: 0.194\n",
            "[8,    40] loss: 0.491\n",
            "[8,    50] loss: 0.333\n",
            "[8,    60] loss: 0.284\n",
            "[8,    70] loss: 0.462\n",
            "[8,    80] loss: 0.148\n",
            "[8,    90] loss: 0.142\n",
            "[8,   100] loss: 0.487\n",
            "[8,   110] loss: 0.234\n",
            "[8,   120] loss: 0.203\n",
            "[8,   130] loss: 0.308\n",
            "[8,   140] loss: 0.264\n",
            "[8,   150] loss: 0.579\n",
            "[8,   160] loss: 0.441\n",
            "[8,   170] loss: 0.175\n",
            "[8,   180] loss: 0.347\n",
            "[8,   190] loss: 0.384\n",
            "[8,   200] loss: 0.503\n",
            "[8,   210] loss: 0.308\n",
            "[8,   220] loss: 0.356\n",
            "[8,   230] loss: 0.103\n",
            "[9,    10] loss: 0.063\n",
            "[9,    20] loss: 0.160\n",
            "[9,    30] loss: 0.216\n",
            "[9,    40] loss: 0.386\n",
            "[9,    50] loss: 0.318\n",
            "[9,    60] loss: 0.585\n",
            "[9,    70] loss: 0.113\n",
            "[9,    80] loss: 0.373\n",
            "[9,    90] loss: 0.354\n",
            "[9,   100] loss: 0.296\n",
            "[9,   110] loss: 0.239\n",
            "[9,   120] loss: 0.344\n",
            "[9,   130] loss: 0.391\n",
            "[9,   140] loss: 0.329\n",
            "[9,   150] loss: 0.144\n",
            "[9,   160] loss: 0.123\n",
            "[9,   170] loss: 0.265\n",
            "[9,   180] loss: 0.247\n",
            "[9,   190] loss: 0.206\n",
            "[9,   200] loss: 0.593\n",
            "[9,   210] loss: 0.330\n",
            "[9,   220] loss: 0.395\n",
            "[9,   230] loss: 0.341\n",
            "[10,    10] loss: 0.290\n",
            "[10,    20] loss: 0.079\n",
            "[10,    30] loss: 0.235\n",
            "[10,    40] loss: 0.270\n",
            "[10,    50] loss: 0.204\n",
            "[10,    60] loss: 0.327\n",
            "[10,    70] loss: 0.194\n",
            "[10,    80] loss: 0.450\n",
            "[10,    90] loss: 0.628\n",
            "[10,   100] loss: 0.347\n",
            "[10,   110] loss: 0.707\n",
            "[10,   120] loss: 0.392\n",
            "[10,   130] loss: 0.505\n",
            "[10,   140] loss: 0.464\n",
            "[10,   150] loss: 0.337\n",
            "[10,   160] loss: 0.134\n",
            "[10,   170] loss: 0.279\n",
            "[10,   180] loss: 0.301\n",
            "[10,   190] loss: 0.449\n",
            "[10,   200] loss: 0.311\n",
            "[10,   210] loss: 0.418\n",
            "[10,   220] loss: 0.233\n",
            "[10,   230] loss: 0.291\n",
            "[11,    10] loss: 0.412\n",
            "[11,    20] loss: 0.302\n",
            "[11,    30] loss: 0.520\n",
            "[11,    40] loss: 0.147\n",
            "[11,    50] loss: 0.072\n",
            "[11,    60] loss: 0.217\n",
            "[11,    70] loss: 0.340\n",
            "[11,    80] loss: 0.340\n",
            "[11,    90] loss: 0.200\n",
            "[11,   100] loss: 0.306\n",
            "[11,   110] loss: 0.237\n",
            "[11,   120] loss: 0.421\n",
            "[11,   130] loss: 0.221\n",
            "[11,   140] loss: 0.262\n",
            "[11,   150] loss: 0.353\n",
            "[11,   160] loss: 0.230\n",
            "[11,   170] loss: 0.113\n",
            "[11,   180] loss: 0.244\n",
            "[11,   190] loss: 0.273\n",
            "[11,   200] loss: 0.228\n",
            "[11,   210] loss: 0.216\n",
            "[11,   220] loss: 0.245\n",
            "[11,   230] loss: 0.136\n",
            "[12,    10] loss: 0.383\n",
            "[12,    20] loss: 0.300\n",
            "[12,    30] loss: 0.304\n",
            "[12,    40] loss: 0.196\n",
            "[12,    50] loss: 0.448\n",
            "[12,    60] loss: 0.215\n",
            "[12,    70] loss: 0.092\n",
            "[12,    80] loss: 0.244\n",
            "[12,    90] loss: 0.326\n",
            "[12,   100] loss: 0.143\n",
            "[12,   110] loss: 0.323\n",
            "[12,   120] loss: 0.341\n",
            "[12,   130] loss: 0.288\n",
            "[12,   140] loss: 0.400\n",
            "[12,   150] loss: 0.486\n",
            "[12,   160] loss: 0.249\n",
            "[12,   170] loss: 0.388\n",
            "[12,   180] loss: 0.105\n",
            "[12,   190] loss: 0.630\n",
            "[12,   200] loss: 0.227\n",
            "[12,   210] loss: 0.406\n",
            "[12,   220] loss: 0.257\n",
            "[12,   230] loss: 0.250\n",
            "[13,    10] loss: 0.339\n",
            "[13,    20] loss: 0.292\n",
            "[13,    30] loss: 0.184\n",
            "[13,    40] loss: 0.243\n",
            "[13,    50] loss: 0.302\n",
            "[13,    60] loss: 0.341\n",
            "[13,    70] loss: 0.339\n",
            "[13,    80] loss: 0.203\n",
            "[13,    90] loss: 0.332\n",
            "[13,   100] loss: 0.343\n",
            "[13,   110] loss: 0.343\n",
            "[13,   120] loss: 0.314\n",
            "[13,   130] loss: 0.267\n",
            "[13,   140] loss: 0.221\n",
            "[13,   150] loss: 0.301\n",
            "[13,   160] loss: 0.318\n",
            "[13,   170] loss: 0.372\n",
            "[13,   180] loss: 0.356\n",
            "[13,   190] loss: 0.195\n",
            "[13,   200] loss: 0.289\n",
            "[13,   210] loss: 0.340\n",
            "[13,   220] loss: 0.326\n",
            "[13,   230] loss: 0.119\n",
            "[14,    10] loss: 0.174\n",
            "[14,    20] loss: 0.267\n",
            "[14,    30] loss: 0.504\n",
            "[14,    40] loss: 0.216\n",
            "[14,    50] loss: 0.122\n",
            "[14,    60] loss: 0.424\n",
            "[14,    70] loss: 0.167\n",
            "[14,    80] loss: 0.248\n",
            "[14,    90] loss: 0.234\n",
            "[14,   100] loss: 0.138\n",
            "[14,   110] loss: 0.295\n",
            "[14,   120] loss: 0.270\n",
            "[14,   130] loss: 0.374\n",
            "[14,   140] loss: 0.286\n",
            "[14,   150] loss: 0.201\n",
            "[14,   160] loss: 0.426\n",
            "[14,   170] loss: 0.205\n",
            "[14,   180] loss: 0.239\n",
            "[14,   190] loss: 0.205\n",
            "[14,   200] loss: 0.235\n",
            "[14,   210] loss: 0.144\n",
            "[14,   220] loss: 0.208\n",
            "[14,   230] loss: 0.317\n",
            "[15,    10] loss: 0.157\n",
            "[15,    20] loss: 0.302\n",
            "[15,    30] loss: 0.208\n",
            "[15,    40] loss: 0.219\n",
            "[15,    50] loss: 0.279\n",
            "[15,    60] loss: 0.286\n",
            "[15,    70] loss: 0.208\n",
            "[15,    80] loss: 0.195\n",
            "[15,    90] loss: 0.455\n",
            "[15,   100] loss: 0.124\n",
            "[15,   110] loss: 0.201\n",
            "[15,   120] loss: 0.067\n",
            "[15,   130] loss: 0.267\n",
            "[15,   140] loss: 0.378\n",
            "[15,   150] loss: 0.277\n",
            "[15,   160] loss: 0.544\n",
            "[15,   170] loss: 0.391\n",
            "[15,   180] loss: 0.411\n",
            "[15,   190] loss: 0.277\n",
            "[15,   200] loss: 0.226\n",
            "[15,   210] loss: 0.396\n",
            "[15,   220] loss: 0.252\n",
            "[15,   230] loss: 0.111\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6S6nMMA88gUz",
        "colab_type": "code",
        "outputId": "1dea40c3-54e6-450a-8e9a-c5705b99cd29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# evaluate the model on test data\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    #iterating through test data\n",
        "    for i, data in enumerate(testDataloader, 0):\n",
        "        \n",
        "        # get the inputs\n",
        "        inputs, labels = data['image'],data['label']\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model_features(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        \n",
        "        #calculating accuracy\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        \n",
        "print('Network Accuracy on test images: %d %%' % (100 * correct / total))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the 800 test images: 91 %\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}